{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c82799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import naive_bayes #import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,average_precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596f8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc35e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_32212/1144886652.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date received', 'Product', 'Sub-product', 'Issue',\n",
      "       'Sub-issue', 'Consumer complaint narrative', 'Company public response',\n",
      "       'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'narr_len',\n",
      "       'days_to_today', 'dupi_id', 'dupi_len'],\n",
      "      dtype='object')\n",
      "(1300361, 23)\n",
      "(1106587, 23)\n"
     ]
    }
   ],
   "source": [
    "cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n",
    "print(cfpb_df.columns)\n",
    "print(cfpb_df.shape)\n",
    "cfpb_df = cfpb_df.drop_duplicates(subset='dupi_id')\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dfa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate y based on 'product' column\n",
    "cfpb_df['debt_collection'] = (cfpb_df['Product'] == 'Debt collection').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset train, dev, test\n",
    "train_df, dev_df, test_df = np.split(cfpb_df[['Consumer complaint narrative','debt_collection']].sample(len(cfpb_df), random_state = 42), \n",
    "                                     [int(len(cfpb_df)*0.75), int(len(cfpb_df)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eccfed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer_999.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "len(loaded_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a3ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorize the text data with pre-tuned vectorizer\n",
    "X_train = loaded_vectorizer.transform(train_df['Consumer complaint narrative'])\n",
    "y_train = train_df['debt_collection']\n",
    "\n",
    "X_dev = loaded_vectorizer.transform(dev_df['Consumer complaint narrative'])\n",
    "y_dev = dev_df['debt_collection']\n",
    "\n",
    "X_test = loaded_vectorizer.transform(test_df['Consumer complaint narrative'])\n",
    "y_test = test_df['debt_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8ccd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 335 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select top 10000 features, 5000 runs faster without significant loss (almost the same)\n",
    "selector = SelectKBest(chi2, k=5000)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_dev = selector.transform(X_dev)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c453728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 247 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# some balancing\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d060940",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e83770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13677/13677 [01:07<00:00, 201.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# traning Gaussian Naive Bayes\n",
    "gnb = naive_bayes.GaussianNB()\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 100\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        gnb.partial_fit(batch_X.A, batch_y, classes=np.unique(y_train_res)) \n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        gnb.partial_fit(batch_X.A, batch_y)\n",
    "\n",
    "# # if you have enough RAM resources, just go big! For 17k features you need about 60+ GB of RAM\n",
    "# gnb.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed8226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13677/13677 [03:14<00:00, 70.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.7501553623781776\n",
      "balanced_accuracy_score 0.7501553623781776\n",
      "average_precision_score 0.7127655035332647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78    683885\n",
      "           1       0.85      0.61      0.71    683885\n",
      "\n",
      "    accuracy                           0.75   1367770\n",
      "   macro avg       0.77      0.75      0.74   1367770\n",
      "weighted avg       0.77      0.75      0.74   1367770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "# again, if you have 117 GB of memorymyou can run this.\n",
    "# y_pred = gnb.predict(X_train_res.toarray())\n",
    "# y_pred_proba = gnb.predict_proba(X_train_res.toarray())\n",
    "\n",
    "# Define batch size and number of batches\n",
    "batch_size = 100\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# Initialize lists to hold batch predictions\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "# Iterate over each batch\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "\n",
    "    # Predict on the batch and append to list\n",
    "    batch_pred = gnb.predict(batch_X.A)\n",
    "    batch_pred_proba = gnb.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "    y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Handling the remaining data\n",
    "if X_train_res.shape[0] % batch_size != 0:\n",
    "    start = n_batches * batch_size\n",
    "    batch_X = X_train_res[start:]\n",
    "\n",
    "    batch_pred = gnb.predict(batch_X.A)\n",
    "    batch_pred_proba = gnb.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "    y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_res, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_res, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebee53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n",
      "accuracy_score 0.8418861604453334\n",
      "balanced_accuracy_score 0.7492392350592455\n",
      "average_precision_score 0.4009413241527506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90    136701\n",
      "           1       0.55      0.61      0.57     29287\n",
      "\n",
      "    accuracy                           0.84    165988\n",
      "   macro avg       0.73      0.75      0.74    165988\n",
      "weighted avg       0.85      0.84      0.85    165988\n",
      "\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# development validation\n",
    "y_pred = gnb.predict(X_dev.toarray())\n",
    "y_pred_proba = gnb.predict_proba(X_dev.toarray())\n",
    "\n",
    "\n",
    "# # Initialize lists to hold batch predictions\n",
    "# y_pred = []\n",
    "# y_pred_proba = []\n",
    "\n",
    "# # Iterate over each batch\n",
    "# for i in tqdm(range(n_batches)):\n",
    "#     start = i * batch_size\n",
    "#     end = (i + 1) * batch_size\n",
    "#     batch_X = X_train_res[start:end]\n",
    "\n",
    "#     # Predict on the batch and append to list\n",
    "#     batch_pred = gnb.predict(batch_X.A)\n",
    "#     batch_pred_proba = gnb.predict_proba(batch_X.A)\n",
    "\n",
    "#     y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# # Convert lists to arrays for further use\n",
    "# y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Dev\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229ef59",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef17cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13677/13677 [00:34<00:00, 397.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# traning Gaussian Naive Bayes\n",
    "mnb = naive_bayes.MultinomialNB()\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 100\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        mnb.partial_fit(batch_X.A, batch_y, classes=np.unique(y_train_res)) \n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        mnb.partial_fit(batch_X.A, batch_y)\n",
    "\n",
    "# # if you have enough RAM resources, just go big! For 17k features you need about 60+ GB of RAM\n",
    "# mnb.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b9e1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13677/13677 [00:55<00:00, 245.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.8275499535740658\n",
      "balanced_accuracy_score 0.8275499535740658\n",
      "average_precision_score 0.7771984718338317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83    683885\n",
      "           1       0.85      0.80      0.82    683885\n",
      "\n",
      "    accuracy                           0.83   1367770\n",
      "   macro avg       0.83      0.83      0.83   1367770\n",
      "weighted avg       0.83      0.83      0.83   1367770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "# y_pred = mnb.predict(X_train_res.toarray())\n",
    "# y_pred_proba = mnb.predict_proba(X_train_res.toarray())\n",
    "\n",
    "# Define batch size and number of batches\n",
    "batch_size = 100\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# Initialize lists to hold batch predictions\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "# Iterate over each batch\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "\n",
    "    # Predict on the batch and append to list\n",
    "    batch_pred = mnb.predict(batch_X.A)\n",
    "    batch_pred_proba = mnb.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "    y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Handling the remaining data\n",
    "if X_train_res.shape[0] % batch_size != 0:\n",
    "    start = n_batches * batch_size\n",
    "    batch_X = X_train_res[start:]\n",
    "\n",
    "    batch_pred = mnb.predict(batch_X.A)\n",
    "    batch_pred_proba = mnb.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "    y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_res, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_res, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aebf6503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n",
      "accuracy_score 0.8469407427042919\n",
      "balanced_accuracy_score 0.829926029321847\n",
      "average_precision_score 0.47256948695016426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90    136701\n",
      "           1       0.54      0.80      0.65     29287\n",
      "\n",
      "    accuracy                           0.85    165988\n",
      "   macro avg       0.75      0.83      0.78    165988\n",
      "weighted avg       0.88      0.85      0.86    165988\n",
      "\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# development validation\n",
    "y_pred = mnb.predict(X_dev.toarray())\n",
    "y_pred_proba = mnb.predict_proba(X_dev.toarray())\n",
    "\n",
    "print(\"Dev\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9686085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25390684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
