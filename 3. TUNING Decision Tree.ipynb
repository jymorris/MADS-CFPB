{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c82799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn import naive_bayes #import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier # simulate the behavior of logistic regression using SGDClassifier(loss='log')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,average_precision_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596f8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f676c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_8524/2270551363.py:6: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv('cfpb_train.csv')\n",
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_8524/2270551363.py:8: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dev_df = pd.read_csv('cfpb_dev.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer_train_split_33k.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "len(loaded_vectorizer.get_feature_names_out())\n",
    "\n",
    "train_df = pd.read_csv('cfpb_train.csv')\n",
    "test_df = pd.read_csv('cfpb_test.csv')\n",
    "dev_df = pd.read_csv('cfpb_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccfed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleaning just ot make sure\n",
    "train_df['Consumer complaint narrative'] = train_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "test_df['Consumer complaint narrative'] = test_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "dev_df['Consumer complaint narrative'] = dev_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "\n",
    "train_df['debt_collection'] = (train_df['Product'] == 'Debt collection').astype(int)\n",
    "test_df['debt_collection'] = (test_df['Product'] == 'Debt collection').astype(int)\n",
    "dev_df['debt_collection'] = (dev_df['Product'] == 'Debt collection').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a3ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = loaded_vectorizer.transform(test_df['Consumer complaint narrative'])\n",
    "y_test = test_df['debt_collection']\n",
    "\n",
    "X_dev = loaded_vectorizer.transform(dev_df['Consumer complaint narrative'])\n",
    "y_dev = dev_df['debt_collection']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765c43b",
   "metadata": {},
   "source": [
    "### Grid Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5405f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score:0.5741186418628853 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 20, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.5755281242046322 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 20, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6209240715130713 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 40, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.624830612376589 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 40, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6294451232885296 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6324026664090426 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__criterion': 'entropy', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6325135538618686 using {'sample_size': 100000, 'chi2_features': 30000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6342158150428054 using {'sample_size': 100000, 'chi2_features': 30000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.6395458538013598 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.6400025475273063 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6412061272812204 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.6414550015678896 using {'sample_size': 200000, 'chi2_features': 25000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.641644908616188 using {'sample_size': 300000, 'chi2_features': 20000, 'clf__criterion': 'entropy', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 50, 'clf__class_weight': None}\n",
      "New best score:0.6423906847976549 using {'sample_size': 300000, 'chi2_features': 30000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 50, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "New best score:0.6441416717786459 using {'sample_size': 300000, 'chi2_features': 30000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "Best parameters: {'sample_size': 300000, 'chi2_features': 30000, 'clf__criterion': 'gini', 'clf__max_depth': 60, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 20, 'clf__class_weight': None}\n",
      "Wall time: 6h 26min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Define the parameters for exploration\n",
    "# # First Run\n",
    "param_grid = {\n",
    "    'sample_size': [100000, 200000, 300000], \n",
    "    'chi2_features': [20000, 25000, 30000], \n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [20, 40, 60],\n",
    "    'clf__min_samples_split': [50, 100],\n",
    "    'clf__min_samples_leaf': [20, 50],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "def grid_search_model(param_grid, train_df, X_dev, y_dev):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    prev_score = 0\n",
    "    \n",
    "    for sample_size in param_grid['sample_size']:\n",
    "        for chi2_features in param_grid['chi2_features']:\n",
    "            # Sample and transform the train data\n",
    "            train_df_sample = train_df.sample(sample_size)\n",
    "            \n",
    "            X_train = loaded_vectorizer.transform(train_df_sample['Consumer complaint narrative'])\n",
    "            y_train = train_df_sample['debt_collection']\n",
    "            \n",
    "            selector = SelectKBest(chi2, k=chi2_features)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            # Transform dev set with the same selector\n",
    "            X_dev_transformed = selector.transform(X_dev)\n",
    "            \n",
    "            oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "            \n",
    "            # Shuffle your data\n",
    "            X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "            \n",
    "            for params in product(\n",
    "                param_grid['clf__criterion'],\n",
    "                param_grid['clf__max_depth'],\n",
    "                param_grid['clf__min_samples_split'],\n",
    "                param_grid['clf__min_samples_leaf'],\n",
    "                param_grid['clf__class_weight']\n",
    "            ):\n",
    "                \n",
    "                criterion, max_depth, min_samples_split, min_samples_leaf, class_weight = params\n",
    "\n",
    "                clf = DecisionTreeClassifier(\n",
    "                    criterion=criterion, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split, \n",
    "                    min_samples_leaf=min_samples_leaf, \n",
    "                    class_weight=class_weight,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                \n",
    "                # Train and score the model\n",
    "                clf.fit(X_train, y_train)\n",
    "                predicted = clf.predict(X_dev_transformed)\n",
    "                score = f1_score(y_dev, predicted)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {\n",
    "                        'sample_size': sample_size,\n",
    "                        'chi2_features': chi2_features,\n",
    "                        'clf__criterion': criterion,\n",
    "                        'clf__max_depth': max_depth,\n",
    "                        'clf__min_samples_split': min_samples_split,\n",
    "                        'clf__min_samples_leaf': min_samples_leaf,\n",
    "                        'clf__class_weight': class_weight\n",
    "                    }\n",
    "                    print(f\"New best score:{score} using {best_params}\")\n",
    "#                 else:\n",
    "#                     print(\"*\", score,sample_size,chi2_features)\n",
    "#                     print(\"*\",params)\n",
    "\n",
    "                prev_score = score\n",
    "                    \n",
    "    return best_params\n",
    "\n",
    "best_params = grid_search_model(param_grid, train_df, X_dev, y_dev)\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c019f",
   "metadata": {},
   "source": [
    "### Train Descision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca2b019",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "class_weight must be dict, 'balanced', or None, got: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \"\"\"\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    970\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                 expanded_class_weight = compute_sample_weight(\n\u001b[0m\u001b[0;32m    228\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_original\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36mcompute_sample_weight\u001b[1;34m(class_weight, y, indices)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mclasses_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_full\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_subsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m             weight_k = compute_class_weight(\n\u001b[0m\u001b[0;32m    181\u001b[0m                 \u001b[0mclass_weight_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_full\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[1;34m\"class_weight must be dict, 'balanced', or None, got: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: class_weight must be dict, 'balanced', or None, got: None"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Assign the values from the dictionary to the variables\n",
    "sample_size = best_params['sample_size']\n",
    "chi2_features = best_params['chi2_features']\n",
    "criterion=best_params['clf__criterion'], \n",
    "max_depth=best_params['clf__max_depth'], \n",
    "min_samples_split=best_params['clf__min_samples_split'], \n",
    "min_samples_leaf=best_params['clf__min_samples_leaf'], \n",
    "class_weight=best_params['clf__class_weight'],\n",
    "\n",
    "# Continue with the rest of your code as it was before\n",
    "train_df_sample = train_df.sample(sample_size)\n",
    "\n",
    "X_train = loaded_vectorizer.transform(train_df_sample['Consumer complaint narrative'])\n",
    "y_train = train_df_sample['debt_collection']\n",
    "\n",
    "selector = SelectKBest(chi2, k=chi2_features)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "# Transform dev set with the same selector\n",
    "X_dev_transformed = selector.transform(X_dev)\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "                    criterion=criterion, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split, \n",
    "                    min_samples_leaf=min_samples_leaf, \n",
    "                    class_weight=class_weight,\n",
    "                    random_state=42\n",
    "                )\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee531a51",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassifier' object has no attribute 'tree_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8524/3881637325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Convert lists to arrays for further use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'tree_'"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_train.toarray())\n",
    "y_pred_proba = clf.predict_proba(X_train.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "c = Counter(y_pred)\n",
    "print(\"Prediction\", c.most_common(2))\n",
    "c = Counter(y_train)\n",
    "print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "prediction = pd.DataFrame(y_pred_proba)\n",
    "prediction['result'] = y_pred\n",
    "\n",
    "df = prediction.copy()\n",
    "df.columns = ['neg', 'pos', 'class']\n",
    "df['true'] = y_train\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5)) # Adjust figsize as needed\n",
    "\n",
    "# Plot Probability Density for Different True Classes\n",
    "for class_label in df['true'].unique():\n",
    "    sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label, ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Probability Density for Different True Classes')\n",
    "axs[0].set_xlabel('Probability')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].legend(title='True Class')\n",
    "\n",
    "# Plot Probability Density for Different Predicted Classes\n",
    "for class_label in df['class'].unique():\n",
    "    sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label, ax=axs[1])\n",
    "\n",
    "axs[1].set_title('Probability Density for Different Predicted Classes')\n",
    "axs[1].set_xlabel('Probability')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend(title='Predicted Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc830345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_dev_transformed.toarray())\n",
    "y_pred_proba = clf.predict_proba(X_dev_transformed.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "\n",
    "c = Counter(y_pred)\n",
    "print(\"Prediction\", c.most_common(2))\n",
    "c = Counter(y_dev)\n",
    "print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "prediction = pd.DataFrame(y_pred_proba)\n",
    "prediction['result'] = y_pred\n",
    "\n",
    "df = prediction.copy()\n",
    "df.columns = ['neg', 'pos', 'class']\n",
    "df['true'] = y_dev\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5)) # Adjust figsize as needed\n",
    "\n",
    "# Plot Probability Density for Different True Classes\n",
    "for class_label in df['true'].unique():\n",
    "    sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label, ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Probability Density for Different True Classes')\n",
    "axs[0].set_xlabel('Probability')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].legend(title='True Class')\n",
    "\n",
    "# Plot Probability Density for Different Predicted Classes\n",
    "for class_label in df['class'].unique():\n",
    "    sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label, ax=axs[1])\n",
    "\n",
    "axs[1].set_title('Probability Density for Different Predicted Classes')\n",
    "axs[1].set_xlabel('Probability')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend(title='Predicted Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669df574",
   "metadata": {},
   "source": [
    "### Descision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714e5e6",
   "metadata": {},
   "source": [
    "New best score:0.6332184473986542 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6341695985269457 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__min_samples_split': 500, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6642955298682403 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6673555720605805 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6685215140731156 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__min_samples_split': 500, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6770409536323196 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6806292706181472 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6815976585490003 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 500, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6831891066429117 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 100, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6837444052947339 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 100, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6844095589169951 using {'sample_size': 100000, 'chi2_features': 20000, 'clf__n_estimators': 100, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 500, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6845885093167702 using {'sample_size': 100000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6881061598951508 using {'sample_size': 100000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': True}\n",
    "\n",
    "New best score:0.6889032466483604 using {'sample_size': 100000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6906264553323377 using {'sample_size': 100000, 'chi2_features': 25000, 'clf__n_estimators': 100, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6908733103203536 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.690875351932429 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__n_estimators': 100, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6916732572590668 using {'sample_size': 200000, 'chi2_features': 20000, 'clf__n_estimators': 100, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6938568507157464 using {'sample_size': 200000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6942864169194196 using {'sample_size': 200000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 30, 'clf__min_samples_split': 500, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.6982193365903832 using {'sample_size': 200000, 'chi2_features': 25000, 'clf__n_estimators': 50, 'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "New best score:0.7001348037225317 using {'sample_size': 300000, 'chi2_features': 30000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "\n",
    "Best parameters: {'sample_size': 300000, 'chi2_features': 30000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f3f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
