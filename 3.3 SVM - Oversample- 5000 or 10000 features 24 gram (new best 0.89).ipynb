{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c82799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression # no partial fit\n",
    "from sklearn.linear_model import SGDClassifier # simulate the behavior of logistic regression using SGDClassifier(loss='log')\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn import naive_bayes #import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,average_precision_score, classification_report\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596f8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc35e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_36692/1144886652.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date received', 'Product', 'Sub-product', 'Issue',\n",
      "       'Sub-issue', 'Consumer complaint narrative', 'Company public response',\n",
      "       'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'narr_len',\n",
      "       'days_to_today', 'dupi_id', 'dupi_len'],\n",
      "      dtype='object')\n",
      "(1300361, 23)\n",
      "(1106587, 23)\n"
     ]
    }
   ],
   "source": [
    "cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n",
    "print(cfpb_df.columns)\n",
    "print(cfpb_df.shape)\n",
    "cfpb_df = cfpb_df.drop_duplicates(subset='dupi_id')\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dfa496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate y based on 'product' column\n",
    "cfpb_df['debt_collection'] = (cfpb_df['Product'] == 'Debt collection').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset train, dev, test\n",
    "train_df, dev_df, test_df = np.split(cfpb_df[['Consumer complaint narrative','debt_collection']].sample(len(cfpb_df), random_state = 42), \n",
    "                                     [int(len(cfpb_df)*0.75), int(len(cfpb_df)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eccfed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16839"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer_999_2to4.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "len(loaded_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a3ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorize the text data with pre-tuned vectorizer\n",
    "X_train = loaded_vectorizer.transform(train_df['Consumer complaint narrative'])\n",
    "y_train = train_df['debt_collection']\n",
    "\n",
    "X_dev = loaded_vectorizer.transform(dev_df['Consumer complaint narrative'])\n",
    "y_dev = dev_df['debt_collection']\n",
    "\n",
    "X_test = loaded_vectorizer.transform(test_df['Consumer complaint narrative'])\n",
    "y_test = test_df['debt_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8ccd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 284 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Select top 10000 features, 5000 runs faster without significant loss (almost the same)\n",
    "# 5000 dev result:  \n",
    "# 1       0.64      0.81      0.72     29287\n",
    "# 1       0.63      0.79      0.70     29287\n",
    "# 10000: \n",
    "# 1       0.63      0.83      0.72     29287\n",
    "# 1       0.63      0.83      0.72     29287 small batch\n",
    "selector = SelectKBest(chi2, k=10000)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_dev = selector.transform(X_dev)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c453728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# some balancing\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d060940",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25390684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [01:01<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle your data\n",
    "X_train_res, y_train_res = shuffle(X_train_res, y_train_res)\n",
    "\n",
    "# Initialize a LogisticRegression model, ‘log_loss’ gives logistic regression, a probabilistic classifier.\n",
    "# ‘hinge’ gives a linear SVM.\n",
    "clf= SGDClassifier(loss='hinge',random_state=42, alpha=1e-4, n_iter_no_change=3, early_stopping=False)\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 50000\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        clf.partial_fit(batch_X.A, batch_y, classes=np.unique(y_train_res))\n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        clf.partial_fit(batch_X.A, batch_y)\n",
    "\n",
    "# # if you have enough RAM resources, just go big! For 17k features you need about 60+ GB of RAM\n",
    "# gnb.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8508cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 136/136 [00:57<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.8696074632430891\n",
      "balanced_accuracy_score 0.8696074632430891\n",
      "average_precision_score 0.8294925937157316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87    683885\n",
      "           1       0.89      0.84      0.87    683885\n",
      "\n",
      "    accuracy                           0.87   1367770\n",
      "   macro avg       0.87      0.87      0.87   1367770\n",
      "weighted avg       0.87      0.87      0.87   1367770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "# again, if you have 117 GB of memorymyou can run this.\n",
    "# y_pred = gnb.predict(X_train_res.toarray())\n",
    "# y_pred_proba = gnb.predict_proba(X_train_res.toarray())\n",
    "\n",
    "# Define batch size and number of batches\n",
    "batch_size = 10000\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# Initialize lists to hold batch predictions\n",
    "y_pred = []\n",
    "# y_pred_proba = []\n",
    "\n",
    "# Iterate over each batch\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "\n",
    "    # Predict on the batch and append to list\n",
    "    batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Handling the remaining data\n",
    "if X_train_res.shape[0] % batch_size != 0:\n",
    "    start = n_batches * batch_size\n",
    "    batch_X = X_train_res[start:]\n",
    "\n",
    "    batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_res, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_res, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e3a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n",
      "accuracy_score 0.8879437067739837\n",
      "balanced_accuracy_score 0.8692138596722139\n",
      "average_precision_score 0.5648471328289228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93    136701\n",
      "           1       0.64      0.84      0.73     29287\n",
      "\n",
      "    accuracy                           0.89    165988\n",
      "   macro avg       0.80      0.87      0.83    165988\n",
      "weighted avg       0.91      0.89      0.89    165988\n",
      "\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# development validation\n",
    "y_pred = clf.predict(X_dev.toarray())\n",
    "# y_pred_proba = clf.predict_proba(X_dev.toarray())\n",
    "\n",
    "\n",
    "# # Initialize lists to hold batch predictions\n",
    "# y_pred = []\n",
    "# y_pred_proba = []\n",
    "\n",
    "# # Define batch size and number of batches\n",
    "# batch_size = 100\n",
    "# n_batches = X_dev.shape[0] // batch_size\n",
    "\n",
    "# # Iterate over each batch\n",
    "# for i in tqdm(range(n_batches)):\n",
    "#     start = i * batch_size\n",
    "#     end = (i + 1) * batch_size\n",
    "#     batch_X = X_dev[start:end]\n",
    "\n",
    "#     # Predict on the batch and append to list\n",
    "#     batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "#     y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# # Convert lists to arrays for further use\n",
    "# y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Dev\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "\n",
    "# c = Counter(y_pred)\n",
    "# print(\"Prediction\", c.most_common(2))\n",
    "# c = Counter(y_dev)\n",
    "# print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "# prediction = pd.DataFrame(y_pred_proba)\n",
    "# prediction['result'] = y_pred\n",
    "# prediction[1].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = prediction.copy()\n",
    "df.columns = ['neg', 'pos', 'class']\n",
    "df['true'] = y_dev\n",
    "\n",
    "# Iterate over each class and plot its density curve\n",
    "for class_label in df['true'].unique():\n",
    "    sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label)\n",
    "\n",
    "plt.title('Probability Density for Different Classes')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each class and plot its density curve\n",
    "for class_label in df['class'].unique():\n",
    "    sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label)\n",
    "\n",
    "plt.title('Probability Density for Different Classes')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdadbbe",
   "metadata": {},
   "source": [
    "### Smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a52a024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2735/2735 [01:09<00:00, 39.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle your data\n",
    "X_train_res, y_train_res = shuffle(X_train_res, y_train_res)\n",
    "\n",
    "# Initialize a LogisticRegression model, ‘log_loss’ gives logistic regression, a probabilistic classifier.\n",
    "clf= SGDClassifier(loss='hinge',random_state=42, alpha=1e-4, n_iter_no_change=3, early_stopping=False)\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 500\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        clf.partial_fit(batch_X.A, batch_y, classes=np.unique(y_train_res)) \n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        clf.partial_fit(batch_X.A, batch_y)\n",
    "\n",
    "# # if you have enough RAM resources, just go big! For 17k features you need about 60+ GB of RAM\n",
    "# gnb.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e88a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 136/136 [00:51<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.8690532765011661\n",
      "balanced_accuracy_score 0.8690532765011661\n",
      "average_precision_score 0.8308890891167484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87    683885\n",
      "           1       0.90      0.83      0.86    683885\n",
      "\n",
      "    accuracy                           0.87   1367770\n",
      "   macro avg       0.87      0.87      0.87   1367770\n",
      "weighted avg       0.87      0.87      0.87   1367770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "# again, if you have 117 GB of memorymyou can run this.\n",
    "# y_pred = gnb.predict(X_train_res.toarray())\n",
    "# y_pred_proba = gnb.predict_proba(X_train_res.toarray())\n",
    "\n",
    "# Define batch size and number of batches\n",
    "batch_size = 10000\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# Initialize lists to hold batch predictions\n",
    "y_pred = []\n",
    "# y_pred_proba = []\n",
    "\n",
    "# Iterate over each batch\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "\n",
    "    # Predict on the batch and append to list\n",
    "    batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Handling the remaining data\n",
    "if X_train_res.shape[0] % batch_size != 0:\n",
    "    start = n_batches * batch_size\n",
    "    batch_X = X_train_res[start:]\n",
    "\n",
    "    batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "    y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_res, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_res, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_res, y_pred))\n",
    "print(classification_report(y_train_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a01b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# development validation\n",
    "y_pred = clf.predict(X_dev.toarray())\n",
    "# y_pred_proba = clf.predict_proba(X_dev.toarray())\n",
    "\n",
    "\n",
    "# # Initialize lists to hold batch predictions\n",
    "# y_pred = []\n",
    "# y_pred_proba = []\n",
    "\n",
    "# # Define batch size and number of batches\n",
    "# batch_size = 100\n",
    "# n_batches = X_dev.shape[0] // batch_size\n",
    "\n",
    "# # Iterate over each batch\n",
    "# for i in tqdm(range(n_batches)):\n",
    "#     start = i * batch_size\n",
    "#     end = (i + 1) * batch_size\n",
    "#     batch_X = X_dev[start:end]\n",
    "\n",
    "#     # Predict on the batch and append to list\n",
    "#     batch_pred = clf.predict(batch_X.A)\n",
    "#     batch_pred_proba = clf.predict_proba(batch_X.A)\n",
    "\n",
    "#     y_pred.extend(batch_pred)\n",
    "#     y_pred_proba.extend(batch_pred_proba)\n",
    "\n",
    "# # Convert lists to arrays for further use\n",
    "# y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a007ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n",
      "accuracy_score 0.891829529845531\n",
      "balanced_accuracy_score 0.8689437216177804\n",
      "average_precision_score 0.5721208267482173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93    136701\n",
      "           1       0.65      0.83      0.73     29287\n",
      "\n",
      "    accuracy                           0.89    165988\n",
      "   macro avg       0.81      0.87      0.83    165988\n",
      "weighted avg       0.91      0.89      0.90    165988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "\n",
    "# c = Counter(y_pred)\n",
    "# print(\"Prediction\", c.most_common(2))\n",
    "# c = Counter(y_dev)\n",
    "# print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "# prediction = pd.DataFrame(y_pred_proba)\n",
    "# prediction['result'] = y_pred\n",
    "# prediction[1].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42374c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = prediction.copy()\n",
    "df.columns = ['neg', 'pos', 'class']\n",
    "df['true'] = y_dev\n",
    "\n",
    "# Iterate over each class and plot its density curve\n",
    "for class_label in df['true'].unique():\n",
    "    sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label)\n",
    "\n",
    "plt.title('Probability Density for Different Classes')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9841ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each class and plot its density curve\n",
    "for class_label in df['class'].unique():\n",
    "    sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label)\n",
    "\n",
    "plt.title('Probability Density for Different Classes')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60a7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957b140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
