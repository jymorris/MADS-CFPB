{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c82799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn import naive_bayes #import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier # simulate the behavior of logistic regression using SGDClassifier(loss='log')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,average_precision_score, classification_report, f1_score\n",
    " \n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596f8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b2e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_31412/2324370275.py:6: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cfpb_df = pd.read_csv('cfpb_train.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer_train_split_33k.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "len(loaded_vectorizer.get_feature_names_out())\n",
    "\n",
    "cfpb_df = pd.read_csv('cfpb_train.csv')\n",
    "# some cleaning just ot make sure\n",
    "cfpb_df['Consumer complaint narrative'] = cfpb_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "cfpb_df['debt_collection'] = (cfpb_df['Product'] == 'Debt collection').astype(int)\n",
    "cv_df = cfpb_df[['Consumer complaint narrative','debt_collection']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c64ad",
   "metadata": {},
   "source": [
    "## 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d4df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2f6fc",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710135a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.8951089588377724\n",
      "balanced_accuracy_score 0.8538980364340482\n",
      "average_precision_score 0.5691223953833455\n",
      "f1_score 0.7270665322580645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94    135990\n",
      "           1       0.67      0.79      0.73     29210\n",
      "\n",
      "    accuracy                           0.90    165200\n",
      "   macro avg       0.81      0.85      0.83    165200\n",
      "weighted avg       0.90      0.90      0.90    165200\n",
      "\n",
      "accuracy_score 0.8970332750198245\n",
      "balanced_accuracy_score 0.8569319842905218\n",
      "average_precision_score 0.5753254956927794\n",
      "f1_score 0.731897991993191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94    135990\n",
      "           1       0.68      0.79      0.73     29209\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.82      0.86      0.83    165199\n",
      "weighted avg       0.91      0.90      0.90    165199\n",
      "\n",
      "accuracy_score 0.8952112300921918\n",
      "balanced_accuracy_score 0.8558553714454658\n",
      "average_precision_score 0.5706578948097143\n",
      "f1_score 0.728467679952316\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94    135989\n",
      "           1       0.67      0.79      0.73     29210\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.81      0.86      0.83    165199\n",
      "weighted avg       0.90      0.90      0.90    165199\n",
      "\n",
      "accuracy_score 0.8959921064897487\n",
      "balanced_accuracy_score 0.8558861328144407\n",
      "average_precision_score 0.5723651167616686\n",
      "f1_score 0.7296642436829353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94    135989\n",
      "           1       0.68      0.79      0.73     29210\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.81      0.86      0.83    165199\n",
      "weighted avg       0.90      0.90      0.90    165199\n",
      "\n",
      "accuracy_score 0.8961373858195267\n",
      "balanced_accuracy_score 0.8550738511889375\n",
      "average_precision_score 0.5721404904610822\n",
      "f1_score 0.7293690851735015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94    135989\n",
      "           1       0.68      0.79      0.73     29210\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.81      0.86      0.83    165199\n",
      "weighted avg       0.90      0.90      0.90    165199\n",
      "\n",
      "Mean F1: 0.7292931066120018 Std F1: 0.001585427755559584\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # Logistic Regression Params:\n",
    "    sample_size = 200000\n",
    "    chi2_features = 22500\n",
    "    alpha = 0.0001\n",
    "    loss = 'log_loss'\n",
    "    penalty=  'elasticnet'\n",
    "    n_iter_no_change = 3\n",
    "    early_stopping = False\n",
    "    learning_rate = 'constant'\n",
    "    eta0 =  0.01\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_train_fold_sample, y_train_fold_sample = oversample.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = SGDClassifier(loss=loss, \n",
    "                        penalty=penalty, \n",
    "                        alpha=alpha,\n",
    "                        n_iter_no_change=n_iter_no_change,\n",
    "                        early_stopping=early_stopping,\n",
    "                        learning_rate=learning_rate,\n",
    "                        eta0=eta0,\n",
    "                        random_state=42)\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2148924",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9f993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.9176937046004843\n",
      "balanced_accuracy_score 0.8498070497712402\n",
      "average_precision_score 0.6259304729207066\n",
      "f1_score 0.7619030941916052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95    135990\n",
      "           1       0.78      0.74      0.76     29210\n",
      "\n",
      "    accuracy                           0.92    165200\n",
      "   macro avg       0.86      0.85      0.86    165200\n",
      "weighted avg       0.92      0.92      0.92    165200\n",
      "\n",
      "accuracy_score 0.9183893364971943\n",
      "balanced_accuracy_score 0.8498357092577018\n",
      "average_precision_score 0.6281607965116728\n",
      "f1_score 0.7631911754373638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95    135990\n",
      "           1       0.78      0.74      0.76     29209\n",
      "\n",
      "    accuracy                           0.92    165199\n",
      "   macro avg       0.86      0.85      0.86    165199\n",
      "weighted avg       0.92      0.92      0.92    165199\n",
      "\n",
      "accuracy_score 0.9160648672207459\n",
      "balanced_accuracy_score 0.8479845123556762\n",
      "average_precision_score 0.6200141493563897\n",
      "f1_score 0.7578075875078599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95    135989\n",
      "           1       0.77      0.74      0.76     29210\n",
      "\n",
      "    accuracy                           0.92    165199\n",
      "   macro avg       0.86      0.85      0.85    165199\n",
      "weighted avg       0.91      0.92      0.92    165199\n",
      "\n",
      "accuracy_score 0.9173179014400814\n",
      "balanced_accuracy_score 0.848812806614096\n",
      "average_precision_score 0.6243152234032313\n",
      "f1_score 0.7605994216107265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95    135989\n",
      "           1       0.78      0.74      0.76     29210\n",
      "\n",
      "    accuracy                           0.92    165199\n",
      "   macro avg       0.86      0.85      0.86    165199\n",
      "weighted avg       0.92      0.92      0.92    165199\n",
      "\n",
      "accuracy_score 0.9175479270455632\n",
      "balanced_accuracy_score 0.8497589632510225\n",
      "average_precision_score 0.6254486939750799\n",
      "f1_score 0.7616080648266448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95    135989\n",
      "           1       0.78      0.74      0.76     29210\n",
      "\n",
      "    accuracy                           0.92    165199\n",
      "   macro avg       0.86      0.85      0.86    165199\n",
      "weighted avg       0.92      0.92      0.92    165199\n",
      "\n",
      "Mean F1: 0.76102186871484 Std F1: 0.0018075475712744742\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # Logistic Regression Params:\n",
    "    sample_size = 150000\n",
    "    chi2_features = 27500\n",
    "    alpha =  0.001\n",
    "    fit_prior =  True\n",
    "    class_prior =  None\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = naive_bayes.MultinomialNB(\n",
    "        alpha=alpha,\n",
    "        fit_prior=fit_prior, \n",
    "        class_prior=class_prior\n",
    "    )\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f105033",
   "metadata": {},
   "source": [
    "### SVM NON-LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca621a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.905817191283293\n",
      "balanced_accuracy_score 0.8822836256839575\n",
      "average_precision_score 0.6116184341435366\n",
      "f1_score 0.760538668718738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94    135990\n",
      "           1       0.69      0.85      0.76     29210\n",
      "\n",
      "    accuracy                           0.91    165200\n",
      "   macro avg       0.83      0.88      0.85    165200\n",
      "weighted avg       0.92      0.91      0.91    165200\n",
      "\n",
      "accuracy_score 0.9079715978910284\n",
      "balanced_accuracy_score 0.8851490945324623\n",
      "average_precision_score 0.6184577167163845\n",
      "f1_score 0.7655630773026569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94    135990\n",
      "           1       0.70      0.85      0.77     29209\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.89      0.85    165199\n",
      "weighted avg       0.92      0.91      0.91    165199\n",
      "\n",
      "accuracy_score 0.9032500196732426\n",
      "balanced_accuracy_score 0.8829689669278158\n",
      "average_precision_score 0.6062413260267387\n",
      "f1_score 0.7568496797651104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94    135989\n",
      "           1       0.68      0.85      0.76     29210\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.82      0.88      0.85    165199\n",
      "weighted avg       0.92      0.90      0.91    165199\n",
      "\n",
      "accuracy_score 0.9075115466800646\n",
      "balanced_accuracy_score 0.8825063823141001\n",
      "average_precision_score 0.6157246202415464\n",
      "f1_score 0.7633914053426248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94    135989\n",
      "           1       0.70      0.84      0.76     29210\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.88      0.85    165199\n",
      "weighted avg       0.92      0.91      0.91    165199\n",
      "\n",
      "accuracy_score 0.9077113057585094\n",
      "balanced_accuracy_score 0.8828293255207179\n",
      "average_precision_score 0.6163998024896818\n",
      "f1_score 0.7638914699869912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94    135989\n",
      "           1       0.70      0.84      0.76     29210\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.88      0.85    165199\n",
      "weighted avg       0.92      0.91      0.91    165199\n",
      "\n",
      "Mean F1: 0.7620468602232242 Std F1: 0.0030613557662182056\n",
      "Wall time: 10min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # SVM Params:\n",
    "    sample_size =400000\n",
    "    chi2_features = 30000\n",
    "    alpha = 0.0001\n",
    "    loss = 'squared_hinge'\n",
    "    penalty=  'elasticnet'\n",
    "    n_iter_no_change = 3\n",
    "    early_stopping = False\n",
    "    learning_rate = 'constant'\n",
    "    eta0 =  0.01\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_train_fold_sample, y_train_fold_sample = oversample.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = SGDClassifier(\n",
    "                    loss=loss, \n",
    "                    penalty=penalty, \n",
    "                    alpha=alpha, \n",
    "                    n_iter_no_change=n_iter_no_change, \n",
    "                    early_stopping=early_stopping, \n",
    "                    learning_rate=learning_rate, \n",
    "                    eta0=eta0, \n",
    "                    random_state=42\n",
    "                )\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2760098",
   "metadata": {},
   "source": [
    "### SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904ab90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.9049818401937046\n",
      "balanced_accuracy_score 0.8592630887257127\n",
      "average_precision_score 0.5953194043288259\n",
      "f1_score 0.7458510758868578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94    135990\n",
      "           1       0.71      0.79      0.75     29210\n",
      "\n",
      "    accuracy                           0.90    165200\n",
      "   macro avg       0.83      0.86      0.84    165200\n",
      "weighted avg       0.91      0.90      0.91    165200\n",
      "\n",
      "accuracy_score 0.9054836893685797\n",
      "balanced_accuracy_score 0.8608818828224665\n",
      "average_precision_score 0.597525618962122\n",
      "f1_score 0.7476484468435853\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94    135990\n",
      "           1       0.71      0.79      0.75     29209\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.86      0.84    165199\n",
      "weighted avg       0.91      0.91      0.91    165199\n",
      "\n",
      "accuracy_score 0.9047815059413192\n",
      "balanced_accuracy_score 0.8611575966246883\n",
      "average_precision_score 0.5959788530253652\n",
      "f1_score 0.7466825560422573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94    135989\n",
      "           1       0.70      0.79      0.75     29210\n",
      "\n",
      "    accuracy                           0.90    165199\n",
      "   macro avg       0.83      0.86      0.84    165199\n",
      "weighted avg       0.91      0.90      0.91    165199\n",
      "\n",
      "accuracy_score 0.9053505166496165\n",
      "balanced_accuracy_score 0.8606967733603139\n",
      "average_precision_score 0.5970954583516309\n",
      "f1_score 0.7473255550887173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94    135989\n",
      "           1       0.71      0.79      0.75     29210\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.86      0.84    165199\n",
      "weighted avg       0.91      0.91      0.91    165199\n",
      "\n",
      "accuracy_score 0.9055260625064316\n",
      "balanced_accuracy_score 0.8598759942595493\n",
      "average_precision_score 0.5970357345940448\n",
      "f1_score 0.7471117232439439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94    135989\n",
      "           1       0.71      0.79      0.75     29210\n",
      "\n",
      "    accuracy                           0.91    165199\n",
      "   macro avg       0.83      0.86      0.84    165199\n",
      "weighted avg       0.91      0.91      0.91    165199\n",
      "\n",
      "Mean F1: 0.7469238714210723 Std F1: 0.0006214130654246049\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # SVM Linear\n",
    "    sample_size =100000\n",
    "    chi2_features = 20000\n",
    "    alpha = 0.0001\n",
    "    loss = 'hinge'\n",
    "    penalty=  'elasticnet'\n",
    "    n_iter_no_change = 3\n",
    "    early_stopping = False\n",
    "    learning_rate = 'adaptive'\n",
    "    eta0 =  0.01\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_train_fold_sample, y_train_fold_sample = oversample.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = SGDClassifier(\n",
    "                    loss=loss, \n",
    "                    penalty=penalty, \n",
    "                    alpha=alpha, \n",
    "                    n_iter_no_change=n_iter_no_change, \n",
    "                    early_stopping=early_stopping, \n",
    "                    learning_rate=learning_rate, \n",
    "                    eta0=eta0, \n",
    "                    random_state=42\n",
    "                )\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b89381",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9c726be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.8526331719128329\n",
      "balanced_accuracy_score 0.7909348523547468\n",
      "average_precision_score 0.4488863674817464\n",
      "f1_score 0.6253174297806848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91    135990\n",
      "           1       0.57      0.70      0.63     29210\n",
      "\n",
      "    accuracy                           0.85    165200\n",
      "   macro avg       0.75      0.79      0.77    165200\n",
      "weighted avg       0.87      0.85      0.86    165200\n",
      "\n",
      "accuracy_score 0.856361115987385\n",
      "balanced_accuracy_score 0.7927509500811115\n",
      "average_precision_score 0.4554442147700546\n",
      "f1_score 0.6309125694109596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91    135990\n",
      "           1       0.58      0.69      0.63     29209\n",
      "\n",
      "    accuracy                           0.86    165199\n",
      "   macro avg       0.75      0.79      0.77    165199\n",
      "weighted avg       0.87      0.86      0.86    165199\n",
      "\n",
      "accuracy_score 0.8617425044945792\n",
      "balanced_accuracy_score 0.7949357354706195\n",
      "average_precision_score 0.46504703272454295\n",
      "f1_score 0.638847601277632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91    135989\n",
      "           1       0.59      0.69      0.64     29210\n",
      "\n",
      "    accuracy                           0.86    165199\n",
      "   macro avg       0.76      0.79      0.78    165199\n",
      "weighted avg       0.87      0.86      0.87    165199\n",
      "\n",
      "accuracy_score 0.8547327768327896\n",
      "balanced_accuracy_score 0.7959870978951671\n",
      "average_precision_score 0.45576351874750043\n",
      "f1_score 0.6318760546096026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91    135989\n",
      "           1       0.57      0.71      0.63     29210\n",
      "\n",
      "    accuracy                           0.85    165199\n",
      "   macro avg       0.75      0.80      0.77    165199\n",
      "weighted avg       0.87      0.85      0.86    165199\n",
      "\n",
      "accuracy_score 0.8558586916385692\n",
      "balanced_accuracy_score 0.7909721379306136\n",
      "average_precision_score 0.4533387736030031\n",
      "f1_score 0.6288421971444603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91    135989\n",
      "           1       0.58      0.69      0.63     29210\n",
      "\n",
      "    accuracy                           0.86    165199\n",
      "   macro avg       0.75      0.79      0.77    165199\n",
      "weighted avg       0.87      0.86      0.86    165199\n",
      "\n",
      "Mean F1: 0.6311591704446678 Std F1: 0.0044537849725405625\n",
      "Wall time: 20min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "{'sample_size': 300000, 'chi2_features': 30000, 'clf__n_estimators': 50, 'clf__criterion': 'entropy', 'clf__max_depth': 50, \n",
    "                                         'clf__min_samples_split': 100, 'clf__min_samples_leaf': 50, 'clf__class_weight': None, 'clf__bootstrap': False}\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # Tree Params:\n",
    "    sample_size = 300000\n",
    "    chi2_features = 30000\n",
    "    criterion='entropy'\n",
    "    max_depth=60\n",
    "    min_samples_split=50\n",
    "    min_samples_leaf=50\n",
    "    class_weight=None\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_train_fold_sample, y_train_fold_sample = oversample.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = DecisionTreeClassifier(\n",
    "                    criterion=criterion, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split, \n",
    "                    min_samples_leaf=min_samples_leaf, \n",
    "                    class_weight=class_weight,\n",
    "                    random_state=42\n",
    "                )\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731ae12",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a90b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.8843523002421307\n",
      "balanced_accuracy_score 0.8243136936962383\n",
      "average_precision_score 0.5264748829073798\n",
      "f1_score 0.6910325867227298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93    135990\n",
      "           1       0.65      0.73      0.69     29210\n",
      "\n",
      "    accuracy                           0.88    165200\n",
      "   macro avg       0.80      0.82      0.81    165200\n",
      "weighted avg       0.89      0.88      0.89    165200\n",
      "\n",
      "accuracy_score 0.8851385298942488\n",
      "balanced_accuracy_score 0.8288732188654833\n",
      "average_precision_score 0.5312346175737108\n",
      "f1_score 0.6954791288857505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93    135990\n",
      "           1       0.65      0.74      0.70     29209\n",
      "\n",
      "    accuracy                           0.89    165199\n",
      "   macro avg       0.80      0.83      0.81    165199\n",
      "weighted avg       0.89      0.89      0.89    165199\n",
      "\n",
      "accuracy_score 0.8828867002826893\n",
      "balanced_accuracy_score 0.8245795079600315\n",
      "average_precision_score 0.5237680227478834\n",
      "f1_score 0.689199826503237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93    135989\n",
      "           1       0.65      0.73      0.69     29210\n",
      "\n",
      "    accuracy                           0.88    165199\n",
      "   macro avg       0.80      0.82      0.81    165199\n",
      "weighted avg       0.89      0.88      0.89    165199\n",
      "\n",
      "accuracy_score 0.886300764532473\n",
      "balanced_accuracy_score 0.8307929276274181\n",
      "average_precision_score 0.5349230678504288\n",
      "f1_score 0.698512062406703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93    135989\n",
      "           1       0.66      0.74      0.70     29210\n",
      "\n",
      "    accuracy                           0.89    165199\n",
      "   macro avg       0.80      0.83      0.81    165199\n",
      "weighted avg       0.89      0.89      0.89    165199\n",
      "\n",
      "accuracy_score 0.8804593248143149\n",
      "balanced_accuracy_score 0.8262367972152247\n",
      "average_precision_score 0.520313417510789\n",
      "f1_score 0.6871157868052474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93    135989\n",
      "           1       0.64      0.74      0.69     29210\n",
      "\n",
      "    accuracy                           0.88    165199\n",
      "   macro avg       0.79      0.83      0.81    165199\n",
      "weighted avg       0.89      0.88      0.88    165199\n",
      "\n",
      "Mean F1: 0.6922678782647336 Std F1: 0.004166170888518208\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_scores = []  # to store the f1_scores for each fold\n",
    "\n",
    "for train_index, val_index in skf.split(cv_df['Consumer complaint narrative'], cv_df['debt_collection']):\n",
    "    # RF Params\n",
    "    sample_size = 300000\n",
    "    chi2_features = 30000\n",
    "    n_estimators = 50\n",
    "    criterion ='entropy'\n",
    "    max_depth = 50\n",
    "    min_samples_split = 100\n",
    "    min_samples_leaf = 50\n",
    "    class_weight = None\n",
    "    bootstrap = False\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    X_train_fold, X_val_fold = cv_df['Consumer complaint narrative'][train_index], cv_df['Consumer complaint narrative'][val_index]\n",
    "    y_train_fold, y_val_fold = cv_df['debt_collection'][train_index], cv_df['debt_collection'][val_index]\n",
    "    \n",
    "    # Further Sampling Since we don't need all that\n",
    "    X_train_fold_sample = X_train_fold.sample(sample_size)\n",
    "    y_train_fold_sample = y_train_fold.loc[X_train_fold_sample.index]  # Get the corresponding labels\n",
    "    \n",
    "    # Vectorize the training and validation sets\n",
    "    X_train_fold_sample = loaded_vectorizer.transform(X_train_fold_sample)\n",
    "    X_val_fold = loaded_vectorizer.transform(X_val_fold)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectKBest(chi2, k=chi2_features)\n",
    "    X_train_fold_sample = selector.fit_transform(X_train_fold_sample, y_train_fold_sample)\n",
    "    X_val_fold = selector.transform(X_val_fold)\n",
    "    \n",
    "    # Oversampling\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_train_fold_sample, y_train_fold_sample = oversample.fit_resample(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Shuffle your data\n",
    "    X_train_fold_sample, y_train_fold_sample = shuffle(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Define the classifier:\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        criterion=criterion, \n",
    "        max_depth=max_depth, \n",
    "        min_samples_split=min_samples_split, \n",
    "        min_samples_leaf=min_samples_leaf, \n",
    "        class_weight=class_weight,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "    clf.fit(X_train_fold_sample, y_train_fold_sample)\n",
    "    \n",
    "    # Predict on the validation fold\n",
    "    y_val_pred = clf.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate the f1_score and append to the list of f1_scores\n",
    "    f1 = f1_score(y_val_fold, y_val_pred)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(\"accuracy_score\",accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"balanced_accuracy_score\",balanced_accuracy_score(y_val_fold, y_val_pred))\n",
    "    print(\"average_precision_score\",average_precision_score(y_val_fold, y_val_pred))\n",
    "    print(\"f1_score\",f1)\n",
    "    print(classification_report(y_val_fold, y_val_pred))\n",
    "    \n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "print('Mean F1:', mean_f1, 'Std F1:', std_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14105a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
