{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a906dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bac054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# for processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# for bag-of-words\n",
    "from sklearn import feature_extraction, feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## for train test split\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ae6e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408b68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_3924/889607879.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date received', 'Product', 'Sub-product', 'Issue',\n",
      "       'Sub-issue', 'Consumer complaint narrative', 'Company public response',\n",
      "       'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'narr_len',\n",
      "       'days_to_today', 'dupi_id', 'dupi_len'],\n",
      "      dtype='object')\n",
      "(1300361, 23)\n"
     ]
    }
   ],
   "source": [
    "cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n",
    "print(cfpb_df.columns)\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0dc6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1106587, 23)\n",
      "Wall time: 455 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Drop duplicates based on 'dupi_id' column\n",
    "cfpb_df = cfpb_df.drop_duplicates(subset='dupi_id')\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be207d97",
   "metadata": {},
   "source": [
    "### Some cleaning and normalizing while trying to keep some the context of data\n",
    "\n",
    "our first try without any normalization did not yield good results, the accuracy was below 50% and the recal for debtcollection is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb5388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## our choices of stop words\n",
    "lst_stopwords = nltk.corpus.stopwords.words('english')\n",
    "#our choice of stemmer\n",
    "stemm=nltk.stem.porter.PorterStemmer()\n",
    "#our choice of lemmatizer\n",
    "lemm=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk_text_normalizer(text):\n",
    "    \n",
    "    '''\n",
    "    Preprocess a string using nltk tools.\n",
    "    you need to import nltk\n",
    "    download nltk.download('wordnet')\n",
    "    and from nltk.stem import WordNetLemmatizer\n",
    "    :parameter\n",
    "        :param text: string - name of column containing text\n",
    "        :param stemm: object - stemmer to be used like nltk.stem.porter.PorterStemmer()\n",
    "        :param lemm: object - lemmatizer() to be used, like WordNetLemmatizer() \n",
    "        :param case_folding - whether text will be convert to lower case\n",
    "        :param lst_stopwords - words to be moved\n",
    "    :return\n",
    "        cleaned text\n",
    "    '''\n",
    "    \n",
    "    ## First we will clean up the XXXX maskings in the text created by CFPB\n",
    "    text = re.sub(r'X{2,}', '', str(text).strip())\n",
    "    \n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = text.lower().strip()\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    ## split is the simplest way\n",
    "    # lst_text = text.split()\n",
    "    ## here we use nltk tools to tokenize a text\n",
    "    lst_text = word_tokenize(text)\n",
    "    \n",
    "    ## remove Stopwords\n",
    "    # lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    lst_text = [lemm.lemmatize(word) for word in lst_text]\n",
    "        \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    ## This stemming parts works pretty bad, but is faster and simplified version of Lemmatization\n",
    "    lst_text = [stemm.stem(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "def normalize_narr(df):\n",
    "    df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(nltk_text_normalizer)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cfpb_df = cfpb_df.groupby(['Product', 'Issue', 'State', 'ZIP code']).apply(func=normalize_narr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681221b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb_df.to_csv('clean_narr_tmp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ab688",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpb_df = pd.read_csv('clean_narr_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset train, dev, test\n",
    "train_df, dev_df, test_df = np.split(cfpb_df.sample(len(cfpb_df), random_state = 42), \n",
    "                                     [int(len(cfpb_df)*0.75), int(len(cfpb_df)*0.9)])\n",
    "train_df.to_csv('cfpb_train.csv', index=False)\n",
    "test_df.to_csv('cfpb_test.csv', index=False)\n",
    "dev_df.to_csv('cfpb_dev.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
