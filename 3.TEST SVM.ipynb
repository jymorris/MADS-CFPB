{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c82799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn import naive_bayes #import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier # simulate the behavior of logistic regression using SGDClassifier(loss='log')\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,average_precision_score, classification_report\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596f8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95fd95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_36832/570579869.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv('cfpb_train.csv')\n",
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_36832/570579869.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dev_df = pd.read_csv('cfpb_dev.csv')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('cfpb_train.csv')\n",
    "test_df = pd.read_csv('cfpb_test.csv')\n",
    "dev_df = pd.read_csv('cfpb_dev.csv')\n",
    "\n",
    "train_df['Consumer complaint narrative'] = train_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "test_df['Consumer complaint narrative'] = test_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "dev_df['Consumer complaint narrative'] = dev_df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "\n",
    "train_df['debt_collection'] = (train_df['Product'] == 'Debt collection').astype(int)\n",
    "test_df['debt_collection'] = (test_df['Product'] == 'Debt collection').astype(int)\n",
    "dev_df['debt_collection'] = (dev_df['Product'] == 'Debt collection').astype(int)\n",
    "\n",
    "train_df_sample = train_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccfed8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer_train_split_33k.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "len(loaded_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a3ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vectorize the text data with pre-tuned vectorizer\n",
    "X_train = loaded_vectorizer.transform(train_df['Consumer complaint narrative'])\n",
    "y_train = train_df['debt_collection']\n",
    "\n",
    "X_train_sample = loaded_vectorizer.transform(train_df_sample['Consumer complaint narrative'])\n",
    "y_train_sample = train_df_sample['debt_collection']\n",
    "\n",
    "X_dev = loaded_vectorizer.transform(dev_df['Consumer complaint narrative'])\n",
    "y_dev = dev_df['debt_collection']\n",
    "\n",
    "X_test = loaded_vectorizer.transform(test_df['Consumer complaint narrative'])\n",
    "y_test = test_df['debt_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8ccd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# selector = SelectKBest(chi2, k=30000)\n",
    "# X_train = selector.fit_transform(X_train, y_train)\n",
    "# X_train_sample = selector.transform(X_train_sample)\n",
    "# X_dev = selector.transform(X_dev)\n",
    "# X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c453728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 780 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# some balancing\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "# # Shuffle your data\n",
    "# X_train_res, y_train_res = shuffle(X_train_res, y_train_res)\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Shuffle your data\n",
    "X_train_res, y_train_res = shuffle(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d060940",
   "metadata": {},
   "source": [
    "### SGD Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e83770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [05:23<00:00, 11.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression model, ‘log_loss’ gives logistic regression, a probabilistic classifier.\n",
    "clf= SGDClassifier(loss='hinge',random_state=42, alpha=1e-4, n_iter_no_change=3, early_stopping=False)\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 50000\n",
    "n_batches = X_train_res.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_res[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        clf.partial_fit(batch_X.A, batch_y, classes=np.unique(y_train_res)) \n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        clf.partial_fit(batch_X.A, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed8226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.9103\n",
      "balanced_accuracy_score 0.8800004452770154\n",
      "average_precision_score 0.617592840990541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94      8253\n",
      "           1       0.71      0.83      0.76      1747\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.83      0.88      0.85     10000\n",
      "weighted avg       0.92      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_train_sample.toarray())\n",
    "# y_pred_proba = clf.predict_proba(X_train_sample.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_sample, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_sample, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_sample, y_pred))\n",
    "print(classification_report(y_train_sample, y_pred))\n",
    "\n",
    "# c = Counter(y_pred)\n",
    "# print(\"Prediction\", c.most_common(2))\n",
    "# c = Counter(y_train_sample)\n",
    "# print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "# prediction = pd.DataFrame(y_pred_proba)\n",
    "# prediction['result'] = y_pred\n",
    "\n",
    "# df = prediction.copy()\n",
    "# df.columns = ['neg', 'pos', 'class']\n",
    "# df['true'] = y_train_sample\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(15, 5)) # Adjust figsize as needed\n",
    "\n",
    "# # Plot Probability Density for Different True Classes\n",
    "# for class_label in df['true'].unique():\n",
    "#     sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label, ax=axs[0])\n",
    "\n",
    "# axs[0].set_title('Probability Density for Different True Classes')\n",
    "# axs[0].set_xlabel('Probability')\n",
    "# axs[0].set_ylabel('Density')\n",
    "# axs[0].legend(title='True Class')\n",
    "\n",
    "# # Plot Probability Density for Different Predicted Classes\n",
    "# for class_label in df['class'].unique():\n",
    "#     sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label, ax=axs[1])\n",
    "\n",
    "# axs[1].set_title('Probability Density for Different Predicted Classes')\n",
    "# axs[1].set_xlabel('Probability')\n",
    "# axs[1].set_ylabel('Density')\n",
    "# axs[1].legend(title='Predicted Class')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebee53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.9080992736077482\n",
      "balanced_accuracy_score 0.8746854623937779\n",
      "average_precision_score 0.6118697432874209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94    136036\n",
      "           1       0.71      0.82      0.76     29164\n",
      "\n",
      "    accuracy                           0.91    165200\n",
      "   macro avg       0.83      0.87      0.85    165200\n",
      "weighted avg       0.92      0.91      0.91    165200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_dev.toarray())\n",
    "# y_pred_proba = clf.predict_proba(X_dev.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "\n",
    "# c = Counter(y_pred)\n",
    "# print(\"Prediction\", c.most_common(2))\n",
    "# c = Counter(y_dev)\n",
    "# print(\"Ground Truth\",c.most_common(2))\n",
    "\n",
    "# prediction = pd.DataFrame(y_pred_proba)\n",
    "# prediction['result'] = y_pred\n",
    "\n",
    "# df = prediction.copy()\n",
    "# df.columns = ['neg', 'pos', 'class']\n",
    "# df['true'] = y_dev\n",
    "\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(15, 5)) # Adjust figsize as needed\n",
    "\n",
    "# # Plot Probability Density for Different True Classes\n",
    "# for class_label in df['true'].unique():\n",
    "#     sns.kdeplot(df[df['true'] == class_label]['pos'], label=class_label, ax=axs[0])\n",
    "\n",
    "# axs[0].set_title('Probability Density for Different True Classes')\n",
    "# axs[0].set_xlabel('Probability')\n",
    "# axs[0].set_ylabel('Density')\n",
    "# axs[0].legend(title='True Class')\n",
    "\n",
    "# # Plot Probability Density for Different Predicted Classes\n",
    "# for class_label in df['class'].unique():\n",
    "#     sns.kdeplot(df[df['class'] == class_label]['pos'], label=class_label, ax=axs[1])\n",
    "\n",
    "# axs[1].set_title('Probability Density for Different Predicted Classes')\n",
    "# axs[1].set_xlabel('Probability')\n",
    "# axs[1].set_ylabel('Density')\n",
    "# axs[1].legend(title='Predicted Class')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16af3e",
   "metadata": {},
   "source": [
    "### Non-Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df940004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d872957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features to have mean=0 and variance=1\n",
    "scaler = StandardScaler(with_mean=False).fit(X_train_res)\n",
    "X_train_scaled = scaler.transform(X_train_res)\n",
    "X_train_sample_scaled = scaler.transform(X_train_sample)\n",
    "X_dev_scaled = scaler.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83854de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Nystroem method to approximate a radial basis function (RBF) kernel\n",
    "nystroem = Nystroem(kernel='rbf', gamma=0.2, random_state=42, n_components=1500)\n",
    "nystroem.fit(X_train_scaled)\n",
    "X_train_transformed = nystroem.transform(X_train_scaled)\n",
    "X_train_sample_transformed = nystroem.transform(X_train_sample_scaled)\n",
    "X_dev_transformed = nystroem.transform(X_dev_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb0f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:18<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a LogisticRegression model, ‘log_loss’ gives logistic regression, a probabilistic classifier.\n",
    "clf= SGDClassifier(loss='hinge',random_state=42, alpha=1e-4, n_iter_no_change=3, early_stopping=False)\n",
    "\n",
    "# The partial fit if you ran out of RAM\n",
    "batch_size = 50000\n",
    "n_batches = X_train_transformed.shape[0] // batch_size\n",
    "\n",
    "# partial fitting\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    batch_X = X_train_transformed[start:end]\n",
    "    batch_y = y_train_res[start:end]\n",
    "    if i == 0:\n",
    "        clf.partial_fit(batch_X, batch_y, classes=np.unique(y_train_res)) \n",
    "        # The `.A` here converts the sparse matrix to a dense matrix.\n",
    "        # This is necessary because GaussianNB doesn't support sparse matrices.\n",
    "        # We also specify the classes parameter in the first call to partial_fit.\n",
    "    else:\n",
    "        clf.partial_fit(batch_X, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b751b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.1871\n",
      "balanced_accuracy_score 0.5072867988334852\n",
      "average_precision_score 0.17682681511138804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.02      0.03      8253\n",
      "           1       0.18      1.00      0.30      1747\n",
      "\n",
      "    accuracy                           0.19     10000\n",
      "   macro avg       0.58      0.51      0.17     10000\n",
      "weighted avg       0.85      0.19      0.08     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_train_sample_transformed)\n",
    "# y_pred_proba = clf.predict_proba(X_train_sample.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_train_sample, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_train_sample, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_train_sample, y_pred))\n",
    "print(classification_report(y_train_sample, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d87f243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "accuracy_score 0.18854721549636805\n",
      "balanced_accuracy_score 0.5071709670194323\n",
      "average_precision_score 0.17864737135014022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.03    136036\n",
      "           1       0.18      1.00      0.30     29164\n",
      "\n",
      "    accuracy                           0.19    165200\n",
      "   macro avg       0.59      0.51      0.17    165200\n",
      "weighted avg       0.85      0.19      0.08    165200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "y_pred = clf.predict(X_dev_transformed)\n",
    "# y_pred_proba = clf.predict_proba(X_dev.toarray())\n",
    "\n",
    "# Convert lists to arrays for further use\n",
    "y_pred = np.array(y_pred)\n",
    "# y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(\"Train\")\n",
    "print(\"accuracy_score\",accuracy_score(y_dev, y_pred))\n",
    "print(\"balanced_accuracy_score\",balanced_accuracy_score(y_dev, y_pred))\n",
    "print(\"average_precision_score\",average_precision_score(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
