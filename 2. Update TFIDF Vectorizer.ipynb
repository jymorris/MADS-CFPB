{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a906dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bac054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JYM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# for processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# for bag-of-words\n",
    "from sklearn import feature_extraction, feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "## for train test split\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ae6e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408b68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JYM\\AppData\\Local\\Temp/ipykernel_30296/889607879.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Date received', 'Product', 'Sub-product', 'Issue',\n",
      "       'Sub-issue', 'Consumer complaint narrative', 'Company public response',\n",
      "       'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n",
      "       'Submitted via', 'Date sent to company', 'Company response to consumer',\n",
      "       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'narr_len',\n",
      "       'days_to_today', 'dupi_id', 'dupi_len'],\n",
      "      dtype='object')\n",
      "(1300361, 23)\n"
     ]
    }
   ],
   "source": [
    "cfpb_df = pd.read_csv('../../data/CFPB with Duplicate Marked.csv')\n",
    "print(cfpb_df.columns)\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0dc6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1106587, 23)\n",
      "Wall time: 510 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Drop duplicates based on 'dupi_id' column\n",
    "cfpb_df = cfpb_df.drop_duplicates(subset='dupi_id')\n",
    "print(cfpb_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848218e",
   "metadata": {},
   "source": [
    "### Round 1! Get all the 1-4-grams from the customer complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249755cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase = False,ngram_range = (1,4))\n",
    "vectorizer.fit(cfpb_df['Consumer complaint narrative'])\n",
    "# This takes about 36GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd05b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabulary and idf score\n",
    "ngrams = vectorizer.get_feature_names_out()\n",
    "# This takes about 31GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the initial filtration\n",
    "%%time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "number_pattern = re.compile(r'^\\d+|\\d+$')  # match numbers at the start or end of a string\n",
    "# regular expression to match repeating characters\n",
    "repeating_chars_pattern = re.compile(r'^(.)\\1*$')\n",
    "\n",
    "filtered_vocab = {}\n",
    "\n",
    "def clean_features(ngram):\n",
    "    words = ngram.split()  # splits the n-gram into individual words\n",
    "    # check if the n-gram starts/ends with a stop word or a number\n",
    "    if (words[0] in stop_words or words[-1] in stop_words) or (number_pattern.match(words[0]) or number_pattern.match(words[-1]) or (repeating_chars_pattern.match(words[0]) or repeating_chars_pattern.match(words[-1]))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ngrams_to_keep = [ngram for ngram in ngrams if clean_features(ngram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ca059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ngrams_to_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cd707",
   "metadata": {},
   "source": [
    "### Round 2! Find proper document frequency thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a new vectorizer with your filtered vocabulary\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=ngrams_to_keep, ngram_range=(1,4))\n",
    "\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the term frequencies\n",
    "tf = np.sum(X, axis=0).A1\n",
    "\n",
    "# Get the document frequencies\n",
    "df = np.sum((X > 0), axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd391c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histograms for Term Frequencies and Document Frequencies\n",
    "fig, axs = plt.subplots(2, figsize=(10, 10))\n",
    "\n",
    "axs[0].hist(tf, bins=10000)\n",
    "axs[0].set_title('Term Frequencies')\n",
    "axs[0].set_yscale('log')  # Using log scale for better visualization\n",
    "axs[0].set_xlim([0, 50000])  # Set x-axis limit to 0-5000\n",
    "\n",
    "\n",
    "axs[1].hist(df, bins=10000)\n",
    "axs[1].set_title('Document Frequencies')\n",
    "axs[1].set_yscale('log')  # Using log scale for better visualization\n",
    "axs[1].set_xlim([0, 50000])  # Set x-axis limit to 0-5000\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histograms for Term Frequencies and Document Frequencies\n",
    "fig, axs = plt.subplots(2, figsize=(10, 10))\n",
    "\n",
    "axs[0].hist(tf, bins=10000)\n",
    "axs[0].set_title('Term Frequencies')\n",
    "axs[0].set_yscale('log')  # Using log scale for better visualization\n",
    "axs[0].set_xlim([0, 5000])  # Set x-axis limit to 0-5000\n",
    "\n",
    "\n",
    "axs[1].hist(df, bins=10000)\n",
    "axs[1].set_title('Document Frequencies')\n",
    "axs[1].set_yscale('log')  # Using log scale for better visualization\n",
    "axs[1].set_xlim([0, 10000])  # Set x-axis limit to 0-5000\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179a2b3",
   "metadata": {},
   "source": [
    "We can see the historgram faltterned out after 800000, this suggest that even the most frequent tokens appeared 0.07 of the total complaints number, suggesting a max_df of 0.08. In this situation, even we set the max_df to 0.8, there won't be much feature can be captured. On the other hand, the histrogram shows a sharp drop after 2000, suggesting a good threshold to drop the rearer n-grams to be 1500 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7884a",
   "metadata": {},
   "source": [
    "### Round 3! Further Reduce n-gram Features By through variance and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53285b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=ngrams_to_keep, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "filtered_vectorizer.fit(cfpb_df['Consumer complaint narrative'])\n",
    "# X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee86c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ngrams = filtered_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976662a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca430a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y = cfpb_df.Product.apply(lambda x: 1 if x==\"Debt collection\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc81f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed07578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Compute chi2 scores and p-values for all features\n",
    "chi2_scores, p_values = chi2(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cefc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of chi2 scores\n",
    "plt.hist(chi2_scores, bins=500, log=True)\n",
    "plt.title('Histogram of Chi2 Scores')\n",
    "plt.xlabel('Chi2 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, plot histogram of p-values\n",
    "plt.hist(p_values, bins=500, log=True)\n",
    "plt.xlim([0.9, 1.0])\n",
    "plt.title('Histogram of P-values')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907871f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in p_values if s>0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in p_values if s>0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in p_values if s>0.997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in p_values if s>0.998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in p_values if s>0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b868fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use p-values to create a boolean mask\n",
    "filtered_feature_names_0995 = np.array(filtered_vectorizer.get_feature_names_out())[p_values >= 0.995]\n",
    "filtered_feature_names_0996 = np.array(filtered_vectorizer.get_feature_names_out())[p_values >= 0.996]\n",
    "filtered_feature_names_0997 = np.array(filtered_vectorizer.get_feature_names_out())[p_values >= 0.997]\n",
    "filtered_feature_names_0998 = np.array(filtered_vectorizer.get_feature_names_out())[p_values >= 0.998]\n",
    "filtered_feature_names_0999 = np.array(filtered_vectorizer.get_feature_names_out())[p_values >= 0.997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_feature_names_0995))\n",
    "print(len(filtered_feature_names_0996))\n",
    "print(len(filtered_feature_names_0997))\n",
    "print(len(filtered_feature_names_0998))\n",
    "print(len(filtered_feature_names_0999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b54b0",
   "metadata": {},
   "source": [
    "### Round 4! Get the TF-IDF vectorizers ready, we will create 5 according for different thresholds and computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78462b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=filtered_feature_names_0995, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])\n",
    "# Save the trained vectorizer\n",
    "with open('tfidf_vectorizer_995.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e777a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=filtered_feature_names_0996, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])\n",
    "# Save the trained vectorizer\n",
    "with open('tfidf_vectorizer_996.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=filtered_feature_names_0997, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])\n",
    "# Save the trained vectorizer\n",
    "with open('tfidf_vectorizer_997.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=filtered_feature_names_0998, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])\n",
    "# Save the trained vectorizer\n",
    "with open('tfidf_vectorizer_998.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vectorizer with your filtered vocabulary, the max_df is actually 0.08 by calculation\n",
    "filtered_vectorizer = TfidfVectorizer(lowercase=False, vocabulary=filtered_feature_names_0999, ngram_range=(1,4), max_df=0.8, min_df=1500)\n",
    "# Fitting the vectorizer and transforming the narratives\n",
    "X = filtered_vectorizer.fit_transform(cfpb_df['Consumer complaint narrative'])\n",
    "# Save the trained vectorizer\n",
    "with open('tfidf_vectorizer_999.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d808ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
